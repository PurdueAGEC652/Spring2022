---
title: "AGEC 652 - Lecture 5.2"
subtitle: "Review of nonlinear estimation: MLE and GMM"
date: "Spring 2022"
author: "Diego S. Cardoso"
#institute: "Purdue University, Department of Agricultural Economics"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---
exclude: true
```{r setup}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  xaringanthemer, JuliaCall
)

#options(htmltools.dir.version = FALSE)

knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo <- FALSE
  }

  knitr::opts_chunk$set(echo = TRUE, fig.align="center")
  options
})

```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#8E6F3E", 
  header_font_google = google_font("Josefin Sans"),
  text_font_size = "28px",
  colors = c(
    red = "#f34213",
    gold = "#CFB991",
    gray = "#C0C0C0",
    blue = "#295fbe",
    black = "#000000"
  )
)

extra_css <- list(
  ".small" = list("font-size" = "90%"),
  ".big" = list("font-size" = "125%"),
  ".footnote" = list("font-size" = "60%"), 
  ".full-width" = list(
    display = "flex",
    width   = "100%",
    flex    = "1 1 auto"
  )
)

style_extra_css(css = extra_css)

```

---

## .blue[Course roadmap]

1. .gray[Intro to Scientific Computing]
2. .gray[Numerical operations and representations]
3. .gray[Systems of equations]
4. .gray[Optimization]
5. **Structural estimation**
  1. .gold[Introduction] 
  2. **Review of estimation methods** $\leftarrow$ .blue[You are here]
  3. .gold[Estimation of single and multiple agent models]


---


class: inverse, center, middle

.footnote[\*These slides are loosely based on Wooldridge (2010) and course materials by Matt Woerman (UMass) and Chris Conlon (NYU Stern).]

---

## Why do we need MLE and GMM?

Sometimes, OLS is all we need
- The parameters we need to estimate are linear in the model
- Or they can be made linear with some clever transformation (like logs)

--

But, in many many cases, we need to estimate parameters that enter nonlinearly in the model
- We need to resort to nonlinear estimators

--

MLE and GMM are the most used estimators for structural modeling and estimation

---

## Maximum Likelihood in one slide

1. We observe some data $(y_i, x_i), i=1,\dots,N$ and assume it comes from a joint distribution described by parameter vector $\theta$

--

2. For any given $\theta$ we can calculate the joint probability of our data
  - If observations are i.i.d., this joint probability is a product of individual probabilities of drawing $(y_i, x_i)$

--

3. Using Bayes' rule, we can calculate the probability of $\theta$ given $(y_i, x_i)$: the **likelihood function**

--

4. The MLE estimate is the value of $\theta$ that maximizes the likelihood: *we pick the parameters that make it most likely to generate the observed data*

---

## Maximum Likelihood intuition

Suppose we draw five numbers from a normal distribution: 
$$y = {47.3, 51.2, 50.5, 44.9, 53.1}$$

--

And we consider two candidate distributions: $N(0,1)$ or $N(50, 1)$

--

- What is the likelihood of $N(0,1)$ generating $y$? 
--
Virtually zero
  
--

- What is the likelihood of $N(50,1)$ generating $y$? 
--
Definitely greater

--

So, between these two, we pick $\mu = 50$, since it's *more likely* to generate $y$ than $\mu = 0$


---

## Maximum Likelihood example

Linear regression: we have $Y_{it} = X_i\beta + \epsilon_i$ and assume $\epsilon_i | X_i \sim N(0, \sigma^2)$

--

With i.i.d. observations, these imply the joint density ($\phi$ is the normal PDF)

$$f(X_1,\dots\,X_N,Y_1,\dots\,Y_N,\beta,\sigma^2) = \prod_{i=1}^N \phi(Y_i-X_i\beta,\sigma^2)$$

--

By Bayes' rule
$$\prod_{i=1}^N \phi(Y_i-X_i\beta,\sigma^2) \propto \prod_{i=1}^N \phi(Y_i-X_i\beta,\sigma^2) = L(\beta,\sigma^2|X,Y)$$


---

## Maximum Likelihood example

Then, we use optimization methods to find
$$(\hat{\beta}_{MLE},\hat{\sigma}^2_{MLE}) = \arg \max_{\beta,\sigma^2} L(\beta,\sigma^2|X,Y)$$

- We can show that this solution is analytically equivalent to OLS

--

- As we saw it in the optimization tutorial, in practice we take logs to transform that product inside $L$ into a sum
  - We maximize the *log-likelihood function* $l(\beta,\sigma^2|X,Y)$

---

## Generalized Method of Moments in one slide

1. Our economic model defines the following population moment conditions: at the true parameter $\theta_0$, $g(x;\theta)$ are on average equal to zero
$$E[g(x;\theta_0)] = 0$$

--

2. We observe some data $x_i, i=1,\dots,N$ and calculate sample analogue 
$$E[g(x;\theta)] \approx \frac{1}{N} \sum_{i=1}^N g(x_i;\theta) \equiv g_N(\theta)$$

--

3. The GMM estimate is given by ( $W_N$ is a weighting matrix)
$$\hat{\theta}_{GMM} = \arg \min_\theta g_N(\theta)^\prime \; W_N \; g_N(\theta)$$

---

## (Generalized) Method of Moments intuition

Suppose we draw five numbers from an unknown distribution
$$y = {47.3, 51.2, 50.5, 44.9, 53.1}$$

--

Suppose this unknown distribution has mean $\mu$, giving a population moment condition

$$E[y_i] = \mu \Rightarrow E[y_i - \mu] = 0$$

--

We expect the population moment condition to also hold in the sample analogue

$$\frac{1}{N}\sum_{i=1}^N (y_i - \mu) = 0$$

---

## (Generalized) Method of Moments intuition

Forget the weighting matrix for now. We have one condition and one parameter, so this is effectively a special case: just *Method of Moments*

--

For our estimate, we pick $\hat{\mu}$ that minimizes

$$\left(\frac{1}{N}\sum_{i=1}^N (y_i - \hat{\mu})\right)^2 $$

In this simple case, this is just solving for $\frac{1}{N}\sum_{i=1}^N (y_i - \hat{\mu}) = 0$


---

## Generalized Method of Moments example

Linear regression: again, we have $Y_{it} = X_i\beta + \epsilon_i$. But now, instead of normality, we assume $\epsilon_i$ is orthogonal to data $X_i$ (a $1 \times K$ vector)
$$E[x_{ki} \epsilon_i] = 0 \Rightarrow E[x_{ki} (Y_i - X_i\beta)] = 0$$
- This actually gives $K$ moment conditions: one for each variable $x_{ki} \in X_i$

--

We replace these $K$ conditions with their respective sample analogues and solve for

$$\frac{1}{N}\sum_{i=1}^N x_{ki} (Y_i - X_i \hat{\beta}) = 0$$

--

- We can show that this solution is analytically equivalent to OLS, too


---

## MLE vs GMM

It's the same old **bias (or robustness) vs. efficiency trade-off**

--

- With MLE, we need to make assumption on distributions of unobservables
  - When our assumptions are correct, MLE is more efficient $\rightarrow$ lower variance
  - Has good small sample properties (less bias, more efficiency with small data)
  - If our assumptions are inadequate, estimates are more biased
  
  
--

- With GMM, we don't need to assume distributions and can rely only on moment conditions from the theoretical and statistical model
  - This is more robust $=$ less bias
  - Has good large sample properties (less bias, more efficiency with large data)
  - But it's in general less efficient than MLE $\rightarrow$ higher variance

---

## Choosing between MLE and GMM

- *How much data is available?*
  - Large data sets favor GMM: good large sample properties requiring fewer assumptions
--

- *How complex is the model?*
  - MLE is better suited for linear and quadratic models, but technically difficult to compute with highly nonlinear models. For the latter case, GMM might be better
--

- *How comfortable are you making distributional assumptions?*
  - MLE requires you to fully specify distributions. If there is good theoretical grounding for these assumptions, MLE is a good idea. Otherwise, GMM is the more attractive option


---

class: inverse, center, middle
# Maximum Likelihood Estimation

---
